{
    "worksheets": [
        {
            "cells": [
                {
                    "source": "# Principles of statistics in Statistical Parametric Mapping with NiPy\n\nIn order to run this tutorial, you'll need to \n[download](http://imaging.mrc-cbu.cam.ac.uk/downloads/Tutscans/egscans.tar.gz) \nand unpack a set of data files.\n\nThe following two commands will download and unpack them on linux:\n\n    wget http://imaging.mrc-cbu.cam.ac.uk/downloads/Tutscans/egscans.tar.gz\n    tar xzf egscans.tar.gz\n\nThe `fetch_scans()` function below will do all the downloading\nin a cross-platform manner, using python-only tools.  You should run it even if you downloaded the files\nmanually, as it will compute the necessary paths for the rest of the script (it won't re-download files\nalready present).", 
                    "cell_type": "markdown"
                }, 
                {
                    "cell_type": "code", 
                    "language": "python", 
                    "outputs": [
                        {
                            "output_type": "stream", 
                            "stream": "stdout", 
                            "text": "All files already present, nothing to download.\n"
                        }
                    ], 
                    "collapsed": false, 
                    "prompt_number": 25, 
                    "input": "import os\nimport tarfile\nimport urllib\n\ndef fetch_scans():\n    # Names of the files in the data bundle\n    scans_dir = 'egscans'\n    files = [os.path.join(scans_dir, 'snn03055dy%i.img' % i) for i in range(1,13)]\n    \n    # The remote url and filename for local storage\n    data_url = 'http://imaging.mrc-cbu.cam.ac.uk/downloads/Tutscans/egscans.tar.gz'\n    tarball = 'egscans.tar.gz'\n\n    # First, check that we don't already have the files, whose names are:\n    if all(map(os.path.isfile, files)):\n        print 'All files already present, nothing to download.'\n        return files\n    \n    # If we don't have them, make the storage directory, fetch (if needed)\n    # and unpack skipping the .mat files that are in the tarball.\n    if not os.path.isdir(scans_dir):\n        os.mkdir(scans_dir)\n\n    if not os.path.isfile(tarball):\n        print \"Fetching remote tarball, this may take some time depending on your network...\"\n        urllib.urlretrieve(data_url, tarball)\n\n    print \"Unpacking scans, omitting .mat files\"\n    tf = tarfile.open(tarball, 'r:gz')\n    image_files = [ f for f in tf.getmembers() if not f.name.endswith('.mat')]\n    tf.extractall(scans_dir, image_files)\n    print \"All files unpacked into\", scans_dir, ', done!'\n    return files\n\nfiles = fetch_scans()"
                }, 
                {
                    "source": "Let's now make actual images out of these files", 
                    "cell_type": "markdown"
                }, 
                {
                    "cell_type": "code", 
                    "language": "python", 
                    "outputs": [
                        {
                            "output_type": "pyerr", 
                            "evalue": "argument 2 to map() must support iteration", 
                            "traceback": [
                                "<span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">TypeError</span>                                 Traceback (most recent call last)", 
                                "<span class=\"ansigreen\">/home/ys218403/Python/notebooks/nipy-notebooks/&lt;ipython-input-26-c9028983ea79&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      1</span> <span class=\"ansigreen\">import</span> nibabel <span class=\"ansigreen\">as</span> nib<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 2</span><span class=\"ansiyellow\"> </span>images <span class=\"ansiyellow\">=</span> map<span class=\"ansiyellow\">(</span>nib<span class=\"ansiyellow\">.</span>load<span class=\"ansiyellow\">,</span> files<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n", 
                                "<span class=\"ansired\">TypeError</span>: argument 2 to map() must support iteration"
                            ], 
                            "ename": "TypeError"
                        }
                    ], 
                    "collapsed": false, 
                    "prompt_number": 26, 
                    "input": "import nibabel as nib\nimages = map(nib.load, files)"
                }, 
                {
                    "input": "# grand mean for data\nGM = 50", 
                    "cell_type": "code", 
                    "collapsed": true, 
                    "language": "python", 
                    "outputs": []
                }, 
                {
                    "source": "Now we load the affine transformation from the first image (it's the same for all) and extract the conversion\nparameters from voxels to MNI space (in mm).", 
                    "cell_type": "markdown"
                }, 
                {
                    "input": "# transformation from voxel no to mm\nM = images[0].get_affine()\n# mm to voxel no\niM = np.linalg.inv(M)\n# coordinates of voxel of interest in mm (MNI space)\nposmm = [-20.0, -42, 34]\n# coordinates in voxel space (in homogenous coordinates)\nposvox = np.dot(iM, posmm + [1])\n# We grab the spatial part of the output.  Since we want to use it as an \n# index, we need to make it a tuple\nposvox = tuple(posvox[:3].astype(int))", 
                    "cell_type": "code", 
                    "collapsed": true, 
                    "language": "python", 
                    "outputs": []
                }, 
                {
                    "source": "We need to define a small utility function commonly used in SPM, we'll call it `global_mean`.", 
                    "cell_type": "markdown"
                }, 
                {
                    "input": "def global_mean(img):\n    \"\"\"Compute a \"sorted global mean\" of an image.\n\n    This returns the mean of the image voxels that are in the range above \n    1/8 of the mean.\"\"\"\n    \n    m = img.mean()/8.0\n    return img[img>m].mean()", 
                    "cell_type": "code", 
                    "collapsed": true, 
                    "language": "python", 
                    "outputs": []
                }, 
                {
                    "source": "With this utility, we can now get the data for the voxel of interest across all the images", 
                    "cell_type": "markdown"
                }, 
                {
                    "input": "nimgs = len(images)\nvdata = np.zeros(nimgs)\ngdata = np.zeros(nimgs)\nfor i, V in enumerate(images):\n    d = V.get_data()\n    vdata[i] = d[posvox]\n    gdata[i] = global_mean(d)", 
                    "cell_type": "code", 
                    "collapsed": true, 
                    "language": "python", 
                    "outputs": []
                }, 
                {
                    "source": "# plot globals", 
                    "cell_type": "markdown"
                }, 
                {
                    "input": "plt.plot(gdata, vdata, 'x')\nplt.xlabel('TMVV for scan')\nplt.ylabel('Voxel value for -20 -42 34')\nplt.title('The relationship of overall signal to values for an individual voxel')\n", 
                    "cell_type": "code", 
                    "collapsed": false, 
                    "language": "python", 
                    "outputs": []
                }, 
                {
                    "input": "# We can check against the current data in the Cambridge wiki tutorial\nfrom IPython.core.display import Image\nImage('http://imaging.mrc-cbu.cam.ac.uk/images/st_globals.gif')", 
                    "cell_type": "code", 
                    "collapsed": false, 
                    "language": "python", 
                    "outputs": []
                }, 
                {
                    "input": "# proportionally scale data\npvdata = vdata / gdata\n# scale to global mean of 50\nY = pvdata * GM\n# our covariate\nourcov = np.array([5,4,4,2,3,1,6,3,1,6,5,2])", 
                    "cell_type": "code", 
                    "collapsed": true, 
                    "language": "python", 
                    "outputs": []
                }, 
                {
                    "input": "# plot data \nplot(ourcov, Y, 'x')\nxlabel('Task difficulty')\nylabel('PS voxel value')\nxlim(0, 7)\ntitle('The relationship of task difficulty to voxel value');", 
                    "cell_type": "code", 
                    "collapsed": false, 
                    "language": "python", 
                    "outputs": []
                }, 
                {
                    "input": "# guess intercept, slope, estimated points\nguessic   = 55\nguesslope = 0.5\nguessPts  = guessic + guesslope*ourcov", 
                    "cell_type": "code", 
                    "collapsed": true, 
                    "language": "python", 
                    "outputs": []
                }, 
                {
                    "source": "For plotting it will be convenient to have the boundaries of our plot region available", 
                    "cell_type": "markdown"
                }, 
                {
                    "input": "minx = 0\nmaxx = ourcov.max()+1 # min max x for plots\naxs = np.array([minx, maxx, Y.min()*0.98, Y.max()*1.02])", 
                    "cell_type": "code", 
                    "collapsed": true, 
                    "language": "python", 
                    "outputs": []
                }, 
                {
                    "input": "# plot data with guess intercept and slope\nplot(ourcov, Y, 'x')\nplot(axs[:2], guessic+guesslope*axs[:2], ':') # guessed line\n# plot guessed residuals\nfor i, c in enumerate(ourcov):\n  plot([c, c], [Y[i], guessPts[i]], color='r')\nplt.axis(axs)\nxlabel('Task difficulty')\nylabel('PS voxel value')\nxlim(0, 7)\ntitle('A first guess at a linear relationship, and its residuals');", 
                    "cell_type": "code", 
                    "collapsed": false, 
                    "language": "python", 
                    "outputs": []
                }, 
                {
                    "input": "# residuals from guess slope\nguessRes = Y - guessPts\n\n# design matrix, df\nX  = np.column_stack([ourcov, ones(nimgs)])\ndf = nimgs - np.linalg.matrix_rank(X)\n\n# mean SoS for guess slope\nguessRSS = (guessRes**2).sum() / df\n\n# show design matrix\nfigure\nplt.gray()\nimshow(X)\ntitle('Design matrix for the first analysis')", 
                    "cell_type": "code", 
                    "collapsed": false, 
                    "language": "python", 
                    "outputs": []
                }, 
                {
                    "input": "# the analysis, giving slope and constant\nB = dot(pinv(X), Y)\n\n# plot data with new least squares line\nplot(ourcov, Y, 'x')\nbound = np.array([minx, maxx])\nplot(bound, bound*B[0]+B[1], 'r')\naxis(axs)\nxlabel('Task difficulty')\nylabel('PS voxel value')\ntitle('The least squares linear relationship');", 
                    "cell_type": "code", 
                    "collapsed": false, 
                    "language": "python", 
                    "outputs": []
                }, 
                {
                    "source": "We'll need some statistical machinery from scipy", 
                    "cell_type": "markdown"
                }, 
                {
                    "input": "from scipy.stats import t as tdist, norm as normdist", 
                    "cell_type": "code", 
                    "collapsed": true, 
                    "language": "python", 
                    "outputs": []
                }, 
                {
                    "source": "And now we can compute our stats for this contrast", 
                    "cell_type": "markdown"
                }, 
                {
                    "input": "# Contrast\nC = np.array([1, 0])\n# t statistic and significance test\nRSS   = ((Y - dot(X, B))**2).sum()\nMRSS  = RSS / df\nSE    = np.sqrt(MRSS*(C.dot(pinv(X.T.dot(X))).dot(C.T)))\nt     = C.dot(B)/SE\nltp   = tdist(df).cdf(t) # lower tail p\nZ     = normdist().ppf(ltp)\np     = 1-ltp           # upper tail p \n\n# print results to matlab window\nprint 'First TD analysis: t= %2.2f, p= %0.6f' % (t, p)", 
                    "cell_type": "code", 
                    "collapsed": false, 
                    "language": "python", 
                    "outputs": []
                }, 
                {
                    "source": "Text and math: $x+1+\\gamma+\\int|\\sum_{i=0}^\\infty$ and displayed\n\n$$x+\\int f(x) \\sum_{i=0}^\\infty$$\n\nand more text", 
                    "cell_type": "markdown"
                }, 
                {
                    "input": "# save data for analysis in e.g. SPSS\nnp.savetxt(\"voxdata.txt\", Y)", 
                    "cell_type": "code", 
                    "collapsed": true, 
                    "language": "python", 
                    "outputs": []
                }, 
                {
                    "source": "now analysis for added covariate\nnote that this and all the other analyses here use\nexactly the same code as above", 
                    "cell_type": "markdown"
                }, 
                {
                    "input": "# design matrix, df\nX = np.column_stack([ourcov, np.arange(1, nimgs+1), np.ones(nimgs)])\ndf = nimgs - np.linalg.matrix_rank(X)\n# show design matrix\nplt.gray()\nplt.imshow(X)\nplt.title('Design matrix for added covariate')", 
                    "cell_type": "code", 
                    "collapsed": false, 
                    "language": "python", 
                    "outputs": []
                }, 
                {
                    "input": "# Contrast\nC = np.array([1, 0, 0])\n# the analysis, giving slope and constant\nB = dot(pinv(X), Y)\n\n# t statistic and significance test\nRSS   = ((Y - dot(X, B))**2).sum()\nMRSS  = RSS / df\nSE    = np.sqrt(MRSS*(C.dot(pinv(X.T.dot(X))).dot(C.T)))\nt     = C.dot(B)/SE\nltp   = tdist(df).cdf(t) # lower tail p\nZ     = normdist().ppf(ltp)\np     = 1-ltp           # upper tail p \n\n# print results to matlab window\nprint 'Added covariate analysis: t= %2.2f, p= %0.6f' % (t, p)", 
                    "cell_type": "code", 
                    "collapsed": false, 
                    "language": "python", 
                    "outputs": []
                }, 
                {
                    "source": "# Conditions analysis", 
                    "cell_type": "markdown"
                }, 
                {
                    "input": "", 
                    "cell_type": "code", 
                    "collapsed": true, 
                    "language": "python", 
                    "outputs": []
                }
            ]
        }
    ], 
    "metadata": {
        "name": "principles_statistics"
    }, 
    "nbformat": 2
}